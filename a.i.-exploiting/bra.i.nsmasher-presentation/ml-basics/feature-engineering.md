<details>

<summary><strong>通过</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS 红队专家)</strong></a><strong>从零到英雄学习 AWS 黑客攻击！</strong></summary>

其他支持 HackTricks 的方式：

* 如果您想在 **HackTricks 中看到您的公司广告** 或 **下载 HackTricks 的 PDF**，请查看 [**订阅计划**](https://github.com/sponsors/carlospolop)！
* 获取 [**官方 PEASS & HackTricks 商品**](https://peass.creator-spring.com)
* 发现 [**PEASS 家族**](https://opensea.io/collection/the-peass-family)，我们独家的 [**NFT 集合**](https://opensea.io/collection/the-peass-family)
* **加入** 💬 [**Discord 群组**](https://discord.gg/hRep4RUj7f) 或 [**telegram 群组**](https://t.me/peass) 或在 **Twitter** 🐦 上 **关注** 我 [**@carlospolopm**](https://twitter.com/carlospolopm)**。**
* **通过向** [**HackTricks**](https://github.com/carlospolop/hacktricks) 和 [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github 仓库提交 PR 来分享您的黑客技巧。

</details>


# 可能数据的基本类型

数据可以是 **连续的**（**无限** 值）或 **分类的**（名义的），其中可能的值数量是 **有限的**。

## 分类类型

### 二进制

只有 **2个可能的值**：1 或 0。如果在数据集中值是字符串格式（例如 "True" 和 "False"），您可以使用以下方法为这些值分配数字：
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **序数**

**值遵循一个顺序**，例如：第1名、第2名……如果类别是字符串（如：“新手”、“业余”、“专业”、“专家”），您可以像我们在二进制案例中看到的那样将它们映射到数字。
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* 对于**字母列**，您可以更容易地对它们进行排序：
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **周期性**

看起来**像序数值**因为有一个顺序，但这并不意味着一个比另一个大。同样，它们之间的**距离取决于你计数的方向**。例如：一周的日子，星期日并不比星期一“大”。

* 有**不同的方法**来编码周期性特征，有些方法可能只适用于某些算法。**通常，可以使用虚拟编码**
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **日期**

日期是**连续的** **变量**。可以被视为**周期性的**（因为它们会重复）**或者**是**序数**变量（因为一个时间点比之前的时间点大）。

* 通常日期被用作**索引**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### 多类别/名义

**超过2个类别**，没有相关顺序。使用 `dataset.describe(include='all')` 来获取每个特征的类别信息。

* **引用字符串**是**标识示例的列**（如一个人的名字）。这可能会重复（因为两个人可能有相同的名字），但大多数将是唯一的。这些数据**无用且应该被移除**。
* **关键列**用于**链接表之间的数据**。在这种情况下，元素是唯一的。这些数据**无用且应该被移除**。

要**将多类别列编码为数字**（以便机器学习算法理解它们），**使用虚拟编码**（而**不是独热编码**，因为它**不能避免完全多重共线性**）。

您可以使用 `pd.get_dummies(dataset.column1)` 获得**独热编码的多类别列**。这将把所有类别转换为二进制特征，因此这将为**每个可能的类别创建一个新列**，并为一个列分配1个**True值**，其余的将是假的。

您可以使用 `pd.get_dummies(dataset.column1, drop_first=True)` 获得**虚拟编码的多类别列**。这将把所有类别转换为二进制特征，因此这将为**每个可能的类别创建一个新列减去一个**，因为**最后两列将在最后创建的二进制列中反映为“1”或“0”**。这将避免完全多重共线性，减少列之间的关系。

# 共线/多重共线性

当**两个特征彼此相关**时，会出现共线。当这些特征超过2个时，会出现多重共线性。

在机器学习中，**您希望您的特征与可能的结果相关，但您不希望它们之间相互相关**。这就是为什么**虚拟编码混合了最后两列**，并且**比独热编码更好**，后者不这样做，从而在多类别列的所有新特征之间创建了明显的关系。

VIF是**方差膨胀因子**，它**衡量特征的多重共线性**。一个**高于5的值意味着应该移除两个或更多共线特征中的一个**。
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# 类别不平衡

这发生在训练数据中**每个类别的数量不相同**时。
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
在不平衡中，总是存在**多数类别**和**少数类别**。

有两种主要方法来解决这个问题：

* **欠采样**：随机移除多数类别中的数据，使其样本数量与少数类别相同。
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Oversampling**：为少数类生成更多数据，直到其样本数量与多数类相同。
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
您可以使用参数 **`sampling_strategy`** 来指定您想要进行**下采样或过采样**的**百分比**（**默认值是 1（100%）**，意味着使少数类的数量与多数类的数量相等）

{% hint style="info" %}
下采样或过采样并不完美，如果您获取过采样或下采样数据的统计信息（使用 `.describe()`）并将其与原始数据进行比较，您会看到**它们发生了变化**。因此，过采样和下采样正在修改训练数据。
{% endhint %}

## SMOTE 过采样

**SMOTE** 通常是一种**更可靠的过采样数据方式**。
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# 罕见类别

想象一个数据集，其中一个目标类别**出现的次数非常少**。

这类似于上一节中的类别不平衡，但罕见类别的出现次数甚至比那种情况下的“少数类”还要少。**原始**的**过采样**和**欠采样**方法也可以在这里使用，但通常这些技术**不会给出真正好的结果**。

## 权重

在某些算法中，可以**修改目标数据的权重**，因此在生成模型时，默认情况下某些数据会更重要。
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
你可以**将权重与过采样/欠采样技术混合**，尝试改善结果。

## PCA - 主成分分析

这是一种帮助降低数据维度的方法。它将**结合不同特征**以**减少**它们的数量，生成**更有用的特征**（_减少了计算需求_）。

结果特征对人类来说是不可理解的，因此它也**匿名化了数据**。

# 不一致的标签类别

数据可能因为转换不成功或仅仅是人为输入错误而出现错误。

因此，你可能会发现**相同的标签有拼写错误**，不同的**大小写**，**缩写**，例如：_BLUE, Blue, b, bule_。在训练模型之前，你需要修正数据中的这些标签错误。

你可以通过将所有内容转换为小写并将拼写错误的标签映射到正确的标签来清理这些问题。

非常重要的是要检查**你拥有的所有数据是否都被正确标记**，因为例如，数据中的一个拼写错误，在对类别进行虚拟编码时，将在最终特征中生成一个新列，**对最终模型产生不良后果**。这个例子可以通过对一列进行独热编码并检查创建的列的名称来很容易地检测到。

# 缺失数据

研究中的某些数据可能缺失。

可能发生某些完全随机的数据因某些错误而缺失。这种数据是**完全随机缺失**（**MCAR**）。

可能某些随机数据缺失，但有某些因素使得某些特定细节更可能缺失，例如，男性更频繁地报告他们的年龄，而女性则不然。这被称为**随机缺失**（**MAR**）。

最后，可能有数据**非随机缺失**（**MNAR**）。数据的价值直接与拥有数据的概率相关。例如，如果你想测量一些尴尬的事情，某人越尴尬，他分享的可能性就越小。

前**两类**缺失数据可以是**可以忽略的**。但是**第三类**需要考虑**只使用未受影响的数据部分**，或尝试**以某种方式对缺失数据进行建模**。

发现缺失数据的一种方法是使用`.info()`函数，因为它会指示**行数但也会指示每个类别的值的数量**。如果某个类别的值少于行数，那么就有一些数据缺失：
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
通常建议，如果一个特征在数据集中**缺失超过20%**，则应该**移除该列：**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
请注意，并非所有缺失值都在数据集中缺失。可能缺失值被赋予了“Unknown”、“n/a”、“”，-1，0等值。您需要检查数据集（使用`dataset.column_name.value_counts(dropna=False)`来检查可能的值）。
{% endhint %}

如果数据集中某些数据缺失（并且数量不多），您需要找到**缺失数据的类别**。为此，您基本上需要知道**数据是否是随机缺失的**，为此您需要找出**缺失数据是否与数据集中的其他数据相关**。

要找出缺失值是否与另一列相关，您可以创建一个新列，如果数据缺失则标记为1，否则为0，然后计算它们之间的相关性：
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
如果您决定忽略缺失数据，您仍然需要处理它：您可以**删除**包含缺失数据的行（模型的训练数据会减少），您可以**完全删除该特征**，或者可以**对其建模**。

您应该**检查缺失特征与目标列之间的相关性**，以了解该特征对目标的重要性，如果它真的**很小**，您可以**删除它或填充它**。

要填补缺失的**连续数据**，您可以使用：**平均值**、**中位数**或使用**插补**算法。插补算法可以尝试使用其他特征来找到缺失特征的值：
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
# 结合特征

如果你发现**两个特征**之间存在**相关性**，通常你应该**删除**其中一个（与目标相关性较低的那个），但你也可以尝试**结合它们并创建一个新特征**。
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>从零开始学习AWS黑客技术，成为</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>！</strong></summary>

支持HackTricks的其他方式：

* 如果您想在**HackTricks中看到您的公司广告**或**下载HackTricks的PDF版本**，请查看[**订阅计划**](https://github.com/sponsors/carlospolop)！
* 获取[**官方PEASS & HackTricks商品**](https://peass.creator-spring.com)
* 发现[**PEASS家族**](https://opensea.io/collection/the-peass-family)，我们独家的[**NFTs系列**](https://opensea.io/collection/the-peass-family)
* **加入** 💬 [**Discord群组**](https://discord.gg/hRep4RUj7f) 或 [**telegram群组**](https://t.me/peass) 或在 **Twitter** 🐦 上**关注**我 [**@carlospolopm**](https://twitter.com/carlospolopm)**。**
* **通过向** [**HackTricks**](https://github.com/carlospolop/hacktricks) 和 [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github仓库提交PR来分享您的黑客技巧。

</details>
