<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>‚òÅÔ∏è HackTricks Cloud ‚òÅÔ∏è</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>üê¶ Twitter üê¶</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>üéôÔ∏è Twitch üéôÔ∏è</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>üé• Youtube üé•</strong></a></summary>

- Travaillez-vous dans une **entreprise de cybers√©curit√©** ? Voulez-vous voir votre **entreprise annonc√©e dans HackTricks** ? ou voulez-vous avoir acc√®s √† la **derni√®re version de PEASS ou t√©l√©charger HackTricks en PDF** ? Consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop) !

- D√©couvrez [**The PEASS Family**](https://opensea.io/collection/the-peass-family), notre collection exclusive de [**NFTs**](https://opensea.io/collection/the-peass-family)

- Obtenez le [**swag officiel PEASS & HackTricks**](https://peass.creator-spring.com)

- **Rejoignez le** [**üí¨**](https://emojipedia.org/speech-balloon/) [**groupe Discord**](https://discord.gg/hRep4RUj7f) ou le [**groupe telegram**](https://t.me/peass) ou **suivez** moi sur **Twitter** [**üê¶**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**.**

- **Partagez vos astuces de piratage en soumettant des PR au [repo hacktricks](https://github.com/carlospolop/hacktricks) et au [repo hacktricks-cloud](https://github.com/carlospolop/hacktricks-cloud)**.

</details>


# Types de donn√©es de base possibles

Les donn√©es peuvent √™tre **continues** (avec des valeurs **infinies**) ou **cat√©gorielles** (nominales) o√π la quantit√© de valeurs possibles est **limit√©e**.

## Types cat√©goriels

### Binaire

Seulement **2 valeurs possibles** : 1 ou 0. Dans le cas o√π dans un ensemble de donn√©es les valeurs sont au format cha√Æne de caract√®res (par exemple "True" et "False"), vous attribuez des nombres √† ces valeurs avec :
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Les **valeurs suivent un ordre**, comme dans : 1√®re place, 2√®me place... Si les cat√©gories sont des cha√Ænes de caract√®res (comme : "d√©butant", "amateur", "professionnel", "expert"), vous pouvez les mapper √† des nombres comme nous l'avons vu dans le cas binaire.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Pour les colonnes **alphab√©tiques**, vous pouvez les trier plus facilement :
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **Cyclique**

Ressemble √† une valeur ordinale car il y a un ordre, mais cela ne signifie pas que l'un est plus grand que l'autre. De plus, la distance entre eux d√©pend de la direction dans laquelle vous comptez. Exemple : Les jours de la semaine, Dimanche n'est pas "plus grand" que Lundi.

* Il existe diff√©rentes fa√ßons de coder les caract√©ristiques cycliques, certaines peuvent fonctionner avec seulement quelques algorithmes. En g√©n√©ral, l'encodage de dummies peut √™tre utilis√©.
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Dates**

Les dates sont des **variables continues**. Elles peuvent √™tre consid√©r√©es comme **cycliques** (car elles se r√©p√®tent) ou comme des variables **ordonn√©es** (car un temps est plus grand qu'un temps pr√©c√©dent).

* Habituellement, les dates sont utilis√©es comme **index**.
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-cat√©gorie/nominale

**Plus de 2 cat√©gories** sans ordre li√©. Utilisez `dataset.describe(include='all')` pour obtenir des informations sur les cat√©gories de chaque fonctionnalit√©.

* Une **cha√Æne de r√©f√©rence** est une **colonne qui identifie un exemple** (comme le nom d'une personne). Cela peut √™tre dupliqu√© (parce que 2 personnes peuvent avoir le m√™me nom), mais la plupart seront uniques. Ces donn√©es sont **inutiles et doivent √™tre supprim√©es**.
* Une **colonne cl√©** est utilis√©e pour **lier des donn√©es entre les tables**. Dans ce cas, les √©l√©ments sont uniques. Ces donn√©es sont **inutiles et doivent √™tre supprim√©es**.

Pour **encoder les colonnes multi-cat√©gories en nombres** (afin que l'algorithme ML les comprenne), l'**encodage de dummies est utilis√©** (et **pas l'encodage one-hot** car cela **n'√©vite pas la multicollin√©arit√© parfaite**).

Vous pouvez obtenir une **colonne multi-cat√©gorie encod√©e en one-hot** avec `pd.get_dummies(dataset.column1)`. Cela transformera toutes les classes en fonctionnalit√©s binaires, cr√©ant ainsi **une nouvelle colonne par classe possible** et attribuera 1 **valeur True √† une colonne**, et le reste sera faux.

Vous pouvez obtenir une **colonne multi-cat√©gorie encod√©e en dummies** avec `pd.get_dummies(dataset.column1, drop_first=True)`. Cela transformera toutes les classes en fonctionnalit√©s binaires, cr√©ant ainsi **une nouvelle colonne par classe possible moins une** car les **deux derni√®res colonnes seront refl√©t√©es comme "1" ou "0" dans la derni√®re colonne binaire cr√©√©e**. Cela √©vitera la multicollin√©arit√© parfaite, r√©duisant les relations entre les colonnes.

# Collin√©aire/Multicollin√©arit√©

La collin√©arit√© appara√Æt lorsque **2 fonctionnalit√©s sont li√©es entre elles**. La multicollin√©arit√© appara√Æt lorsque celles-ci sont plus de 2.

En ML, **vous voulez que vos fonctionnalit√©s soient li√©es aux r√©sultats possibles, mais vous ne voulez pas qu'elles soient li√©es entre elles**. C'est pourquoi l'**encodage de dummies m√©lange les deux derni√®res colonnes** de cela et **est meilleur que l'encodage one-hot** qui ne cr√©e pas cela en cr√©ant une relation claire entre toutes les nouvelles fonctionnalit√©s de la colonne multi-cat√©gorie.

VIF est le **facteur d'inflation de la variance** qui **mesure la multicollin√©arit√© des fonctionnalit√©s**. Une valeur **sup√©rieure √† 5 signifie qu'une des deux ou plusieurs fonctionnalit√©s collin√©aires doit √™tre supprim√©e**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# D√©s√©quilibre cat√©goriel

Cela se produit lorsqu'il n'y a **pas le m√™me nombre de chaque cat√©gorie** dans les donn√©es d'entra√Ænement.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
Dans un d√©s√©quilibre, il y a toujours une ou plusieurs **classes majoritaires** et une ou plusieurs **classes minoritaires**.

Il existe 2 principales fa√ßons de r√©soudre ce probl√®me :

* **Sous-√©chantillonnage** : Supprimer al√©atoirement des donn√©es de la classe majoritaire afin qu'elle ait le m√™me nombre d'√©chantillons que la classe minoritaire.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Sur√©chantillonnage**: G√©n√©rer plus de donn√©es pour la classe minoritaire jusqu'√† ce qu'elle ait autant d'√©chantillons que la classe majoritaire.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Vous pouvez utiliser l'argument **`sampling_strategy`** pour indiquer le **pourcentage** que vous souhaitez **sous-√©chantillonner ou sur-√©chantillonner** (**par d√©faut, c'est 1 (100%)** ce qui signifie √©galiser le nombre de classes minoritaires avec les classes majoritaires).

{% hint style="info" %}
La sous-√©chantillonnage ou la sur-√©chantillonnage ne sont pas parfaites, si vous obtenez des statistiques (avec `.describe()`) des donn√©es sur/sous-√©chantillonn√©es et que vous les comparez √† l'original, vous verrez **qu'elles ont chang√©**. Par cons√©quent, la sur-√©chantillonnage et la sous-√©chantillonnage modifient les donn√©es d'entra√Ænement.
{% endhint %}

## Sur-√©chantillonnage SMOTE

**SMOTE** est g√©n√©ralement une **m√©thode plus fiable pour sur-√©chantillonner les donn√©es**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Cat√©gories rarement pr√©sentes

Imaginez un ensemble de donn√©es o√π l'une des classes cibles **appara√Æt tr√®s peu de fois**.

C'est comme le d√©s√©quilibre de cat√©gorie de la section pr√©c√©dente, mais la cat√©gorie rarement pr√©sente appara√Æt encore moins que la "classe minoritaire" dans ce cas. Les m√©thodes de **sur√©chantillonnage** et de **sous-√©chantillonnage** brutes pourraient √©galement √™tre utilis√©es ici, mais en g√©n√©ral, ces techniques **ne donneront pas de tr√®s bons r√©sultats**.

## Poids

Dans certains algorithmes, il est possible de **modifier les poids des donn√©es cibl√©es** afin que certaines d'entre elles aient par d√©faut plus d'importance lors de la g√©n√©ration du mod√®le.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Vous pouvez **m√©langer les poids avec des techniques de sur/sous-√©chantillonnage** pour essayer d'am√©liorer les r√©sultats.

## PCA - Analyse en Composantes Principales

C'est une m√©thode qui aide √† r√©duire la dimensionnalit√© des donn√©es. Elle va **combiner diff√©rentes caract√©ristiques** pour **r√©duire leur quantit√©** en g√©n√©rant des **caract√©ristiques plus utiles** (_moins de calculs sont n√©cessaires_).

Les caract√©ristiques r√©sultantes ne sont pas compr√©hensibles par les humains, donc cela **anonymise √©galement les donn√©es**.

# Cat√©gories d'√©tiquettes incoh√©rentes

Les donn√©es peuvent comporter des erreurs dues √† des transformations infructueuses ou simplement √† des erreurs humaines lors de la saisie des donn√©es.

Par cons√©quent, vous pouvez trouver la **m√™me √©tiquette avec des fautes d'orthographe**, des **diff√©rences de majuscules**, des **abr√©viations** comme: _BLUE, Blue, b, bule_. Vous devez corriger ces erreurs d'√©tiquetage dans les donn√©es avant de former le mod√®le.

Vous pouvez r√©soudre ces probl√®mes en mettant tout en minuscules et en faisant correspondre les √©tiquettes mal orthographi√©es aux bonnes.

Il est tr√®s important de v√©rifier que **toutes les donn√©es que vous avez contiennent des √©tiquettes correctes**, car par exemple, une erreur d'orthographe dans les donn√©es, lors de l'encodage des classes, g√©n√©rera une nouvelle colonne dans les caract√©ristiques finales avec des **cons√©quences n√©fastes pour le mod√®le final**. Cet exemple peut √™tre d√©tect√© tr√®s facilement en encodant √† chaud une colonne et en v√©rifiant les noms des colonnes cr√©√©es.

# Donn√©es manquantes

Certaines donn√©es de l'√©tude peuvent √™tre manquantes.

Il se peut que certaines donn√©es al√©atoires compl√®tes soient manquantes en raison d'une erreur. Ce type de donn√©es est **manquant compl√®tement au hasard** (**MCAR**).

Il se pourrait que certaines donn√©es al√©atoires soient manquantes, mais qu'il y ait quelque chose qui rend certains d√©tails plus probables d'√™tre manquants, par exemple, les hommes donneront plus fr√©quemment leur √¢ge mais pas les femmes. Cela s'appelle **manquant au hasard** (**MAR**).

Enfin, il pourrait y avoir des donn√©es **manquantes non au hasard** (**MNAR**). La valeur des donn√©es est directement li√©e √† la probabilit√© d'avoir les donn√©es. Par exemple, si vous voulez mesurer quelque chose de g√™nant, plus quelqu'un est g√™n√©, moins il est probable qu'il le partage.

Les **deux premi√®res cat√©gories** de donn√©es manquantes peuvent √™tre **ignor√©es**. Mais la **troisi√®me** n√©cessite de consid√©rer **seulement des portions des donn√©es** qui ne sont pas impact√©es ou d'essayer de **mod√©liser les donn√©es manquantes d'une certaine mani√®re**.

Une fa√ßon de d√©couvrir les donn√©es manquantes est d'utiliser la fonction `.info()` car elle indiquera le **nombre de lignes mais aussi le nombre de valeurs par cat√©gorie**. Si une cat√©gorie a moins de valeurs que le nombre de lignes, alors il manque des donn√©es :
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Il est g√©n√©ralement recommand√© que si une fonctionnalit√© est **manquante dans plus de 20%** de l'ensemble de donn√©es, la **colonne doit √™tre supprim√©e :**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Notez que **toutes les valeurs manquantes ne sont pas manquantes dans l'ensemble de donn√©es**. Il est possible que des valeurs manquantes aient √©t√© donn√©es avec la valeur "Inconnu", "n/a", "", -1, 0... Vous devez v√©rifier l'ensemble de donn√©es (en utilisant `ensemble_de_donn√©es.nom_de_colonne.valeur_comptes(dropna=False)` pour v√©rifier les valeurs possibles).
{% endhint %}

Si des donn√©es sont manquantes dans l'ensemble de donn√©es (et qu'il n'y en a pas trop), vous devez trouver la **cat√©gorie de donn√©es manquantes**. Pour cela, vous devez essentiellement savoir si les **donn√©es manquantes sont al√©atoires ou non**, et pour cela, vous devez trouver si les **donn√©es manquantes √©taient corr√©l√©es avec d'autres donn√©es** de l'ensemble de donn√©es.

Pour savoir si une valeur manquante est corr√©l√©e avec une autre colonne, vous pouvez cr√©er une nouvelle colonne qui met des 1 et des 0 si les donn√©es sont manquantes ou non, puis calculer la corr√©lation entre elles :
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Si vous d√©cidez d'ignorer les donn√©es manquantes, vous devez quand m√™me d√©cider quoi faire avec : vous pouvez **supprimer les lignes** avec des donn√©es manquantes (les donn√©es d'entra√Ænement pour le mod√®le seront plus petites), vous pouvez **supprimer compl√®tement la caract√©ristique**, ou vous pouvez **la mod√©liser**.

Vous devriez **v√©rifier la corr√©lation entre la caract√©ristique manquante et la colonne cible** pour voir √† quel point cette caract√©ristique est importante pour la cible, si elle est vraiment **petite**, vous pouvez **la supprimer ou la remplir**.

Pour remplir les donn√©es continues manquantes, vous pouvez utiliser : la **moyenne**, la **m√©diane** ou utiliser un **algorithme d'imputation**. L'algorithme d'imputation peut essayer d'utiliser d'autres caract√©ristiques pour trouver une valeur pour la caract√©ristique manquante :
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Pour remplir les donn√©es cat√©gorielles, tout d'abord, vous devez r√©fl√©chir s'il y a une raison pour laquelle les valeurs sont manquantes. Si c'est par **choix des utilisateurs** (ils ne voulaient pas donner les donn√©es), vous pouvez peut-√™tre **cr√©er une nouvelle cat√©gorie** l'indiquant. S'il s'agit d'une erreur humaine, vous pouvez **supprimer les lignes** ou la **caract√©ristique** (v√©rifiez les √©tapes mentionn√©es pr√©c√©demment) ou **remplir avec le mode, la cat√©gorie la plus utilis√©e** (non recommand√©).

# Combinaison de caract√©ristiques

Si vous trouvez **deux caract√©ristiques** qui sont **corr√©l√©es** entre elles, vous devriez g√©n√©ralement **supprimer** l'une d'entre elles (celle qui est moins corr√©l√©e avec la cible), mais vous pouvez √©galement essayer de **les combiner et de cr√©er une nouvelle caract√©ristique**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>‚òÅÔ∏è HackTricks Cloud ‚òÅÔ∏è</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>üê¶ Twitter üê¶</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>üéôÔ∏è Twitch üéôÔ∏è</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>üé• Youtube üé•</strong></a></summary>

- Travaillez-vous dans une entreprise de **cybers√©curit√©** ? Voulez-vous voir votre **entreprise annonc√©e dans HackTricks** ? ou voulez-vous avoir acc√®s √† la **derni√®re version de PEASS ou t√©l√©charger HackTricks en PDF** ? Consultez les [**PLANS D'ABONNEMENT**](https://github.com/sponsors/carlospolop) !

- D√©couvrez [**The PEASS Family**](https://opensea.io/collection/the-peass-family), notre collection exclusive de [**NFTs**](https://opensea.io/collection/the-peass-family)

- Obtenez le [**swag officiel PEASS & HackTricks**](https://peass.creator-spring.com)

- **Rejoignez le** [**üí¨**](https://emojipedia.org/speech-balloon/) **groupe Discord** ou le [**groupe telegram**](https://t.me/peass) ou **suivez** moi sur **Twitter** [**üê¶**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**.**

- **Partagez vos astuces de piratage en soumettant des PR au [repo hacktricks](https://github.com/carlospolop/hacktricks) et au [repo hacktricks-cloud](https://github.com/carlospolop/hacktricks-cloud)**.

</details>
