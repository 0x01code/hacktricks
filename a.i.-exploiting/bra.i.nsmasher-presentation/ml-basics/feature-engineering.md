<details>

<summary><strong>Aprende hacking en AWS de cero a h칠roe con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si quieres ver a tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF**, consulta los [**PLANES DE SUSCRIPCI칍N**](https://github.com/sponsors/carlospolop)!
* Consigue el [**merchandising oficial de PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubre [**La Familia PEASS**](https://opensea.io/collection/the-peass-family), nuestra colecci칩n de [**NFTs**](https://opensea.io/collection/the-peass-family) exclusivos
* **칔nete al** 游눫 [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s칤gueme** en **Twitter** 游냕 [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Comparte tus trucos de hacking enviando PRs a los repositorios de github de** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>


# Tipos b치sicos de datos posibles

Los datos pueden ser **continuos** (**infinitos** valores) o **categ칩ricos** (nominales) donde la cantidad de valores posibles es **limitada**.

## Tipos categ칩ricos

### Binario

Solo **2 valores posibles**: 1 o 0. En caso de que en un conjunto de datos los valores est칠n en formato de cadena (por ejemplo, "True" y "False") puedes asignar n칰meros a esos valores con:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Los **valores siguen un orden**, como en: 1er lugar, 2do lugar... Si las categor칤as son cadenas (como: "principiante", "aficionado", "profesional", "experto") puedes mapearlas a n칰meros como vimos en el caso binario.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para **columnas alfab칠ticas** puedes ordenarlas m치s f치cilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C칤clico**

Parece **como valor ordinal** porque hay un orden, pero no significa que uno sea mayor que el otro. Adem치s, la **distancia entre ellos depende de la direcci칩n** en la que est칠s contando. Ejemplo: Los d칤as de la semana, el domingo no es "mayor" que el lunes.

* Hay **diferentes maneras** de codificar caracter칤sticas c칤clicas, algunas pueden funcionar solo con algunos algoritmos. **En general, se puede usar la codificaci칩n dummy**
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Fechas**

Las fechas son **variables** **continuas**. Pueden considerarse **c칤clicas** (porque se repiten) **o** como variables **ordinales** (porque un momento es mayor que el anterior).

* Generalmente las fechas se utilizan como **칤ndice**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categor칤a/nominal

**M치s de 2 categor칤as** sin un orden relacionado. Usa `dataset.describe(include='all')` para obtener informaci칩n sobre las categor칤as de cada caracter칤stica.

* Una **cadena de referencia** es una **columna que identifica un ejemplo** (como el nombre de una persona). Esto puede estar duplicado (porque 2 personas pueden tener el mismo nombre) pero la mayor칤a ser치 칰nico. Estos datos son **in칰tiles y deben eliminarse**.
* Una **columna clave** se utiliza para **vincular datos entre tablas**. En este caso los elementos son 칰nicos. Estos datos son **in칰tiles y deben eliminarse**.

Para **codificar columnas de multi-categor칤a en n칰meros** (para que el algoritmo de ML las entienda), se utiliza la **codificaci칩n dummy** (y **no la codificaci칩n one-hot** porque **no evita la multicolinealidad perfecta**).

Puedes obtener una **columna de multi-categor칤a codificada en one-hot** con `pd.get_dummies(dataset.column1)`. Esto transformar치 todas las clases en caracter칤sticas binarias, por lo que crear치 **una nueva columna por cada clase posible** y asignar치 1 **valor verdadero a una columna**, y el resto ser치n falsos.

Puedes obtener una **columna de multi-categor칤a codificada en dummy** con `pd.get_dummies(dataset.column1, drop_first=True)`. Esto transformar치 todas las clases en caracter칤sticas binarias, por lo que crear치 **una nueva columna por cada clase posible menos una** ya que **las 칰ltimas 2 columnas se reflejar치n como "1" o "0" en la 칰ltima columna binaria creada**. Esto evitar치 la multicolinealidad perfecta, reduciendo las relaciones entre columnas.

# Colineal/Multicolinealidad

La colinealidad aparece cuando **2 caracter칤sticas est치n relacionadas entre s칤**. La multicolinealidad aparece cuando son m치s de 2.

En ML **quieres que tus caracter칤sticas est칠n relacionadas con los posibles resultados pero no quieres que est칠n relacionadas entre ellas**. Por eso la **codificaci칩n dummy mezcla las 칰ltimas dos columnas** de eso y **es mejor que la codificaci칩n one-hot** que no hace eso creando una relaci칩n clara entre todas las nuevas caracter칤sticas de la columna de multi-categor칤a.

VIF es el **Factor de Inflaci칩n de la Varianza** que **mide la multicolinealidad de las caracter칤sticas**. Un valor **por encima de 5 significa que una de las dos o m치s caracter칤sticas colineales debe eliminarse**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequilibrio Categ칩rico

Esto ocurre cuando **no hay la misma cantidad de cada categor칤a** en los datos de entrenamiento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
En un desequilibrio siempre hay una **clase o clases mayoritarias** y una **clase o clases minoritarias**.

Hay 2 maneras principales de solucionar este problema:

* **Undersampling**: Eliminar datos seleccionados al azar de la clase mayoritaria para que tenga el mismo n칰mero de muestras que la clase minoritaria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Oversampling**: Generar m치s datos para la clase minoritaria hasta que tenga tantas muestras como la clase mayoritaria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Puede utilizar el argumento **`sampling_strategy`** para indicar el **porcentaje** que desea **submuestrear o sobremuestrear** (**por defecto es 1 (100%)**, lo que significa igualar el n칰mero de clases minoritarias con las clases mayoritarias)

{% hint style="info" %}
El submuestreo o sobremuestreo no son perfectos si obtiene estad칤sticas (con `.describe()`) de los datos sub/sobremuestreados y los compara con los originales, ver치 **que han cambiado**. Por lo tanto, el sobremuestreo y submuestreo est치n modificando los datos de entrenamiento.
{% endhint %}

## Sobremuestreo con SMOTE

**SMOTE** suele ser una **forma m치s confiable de sobremuestrear los datos**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categor칤as que Ocurren Raramente

Imagina un conjunto de datos donde una de las clases objetivo **ocurre muy pocas veces**.

Esto es como el desequilibrio de categor칤as de la secci칩n anterior, pero la categor칤a que ocurre raramente lo hace incluso menos que la "clase minoritaria" en ese caso. Los m칠todos de **sobremuestreo** y **submuestreo** **crudos** tambi칠n podr칤an usarse aqu칤, pero generalmente esas t칠cnicas **no dar치n resultados realmente buenos**.

## Pesos

En algunos algoritmos es posible **modificar los pesos de los datos objetivo** para que algunos de ellos obtengan por defecto m치s importancia al generar el modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Puedes **combinar los pesos con t칠cnicas de sobremuestreo/submuestreo** para intentar mejorar los resultados.

## PCA - An치lisis de Componentes Principales

Es un m칠todo que ayuda a reducir la dimensionalidad de los datos. Va a **combinar diferentes caracter칤sticas** para **reducir la cantidad** de ellas generando **caracter칤sticas m치s 칰tiles** (_se necesita menos c치lculo_).

Las caracter칤sticas resultantes no son comprensibles para los humanos, por lo que tambi칠n **anonimiza los datos**.

# Categor칤as de Etiquetas Incongruentes

Los datos pueden tener errores por transformaciones fallidas o simplemente por error humano al escribir los datos.

Por lo tanto, podr칤as encontrar la **misma etiqueta con errores ortogr치ficos**, diferente **capitalizaci칩n**, **abreviaturas** como: _BLUE, Blue, b, bule_. Necesitas corregir estos errores de etiquetas dentro de los datos antes de entrenar el modelo.

Puedes limpiar estos problemas convirtiendo todo a min칰sculas y mapeando las etiquetas mal escritas a las correctas.

Es muy importante verificar que **todos los datos que tienes est칠n correctamente etiquetados**, porque por ejemplo, un error de ortograf칤a en los datos, al codificar las clases de forma ficticia, generar치 una nueva columna en las caracter칤sticas finales con **malas consecuencias para el modelo final**. Este ejemplo se puede detectar muy f치cilmente codificando una columna en caliente y revisando los nombres de las columnas creadas.

# Datos Faltantes

Puede que falten algunos datos del estudio.

Podr칤a ocurrir que algunos datos completamente aleatorios falten por alg칰n error. Este tipo de datos es **Missing Completely at Random** (**MCAR**).

Podr칤a ser que falten algunos datos aleatorios pero hay algo que hace que ciertos detalles espec칤ficos sean m치s propensos a faltar, por ejemplo, es m치s frecuente que los hombres revelen su edad pero no las mujeres. Esto se llama **Missing at Random** (**MAR**).

Finalmente, podr칤a haber datos **Missing Not at Random** (**MNAR**). El valor de los datos est치 directamente relacionado con la probabilidad de tener los datos. Por ejemplo, si quieres medir algo vergonzoso, cuanto m치s vergonzoso sea alguien, menos probable es que lo comparta.

Las **dos primeras categor칤as** de datos faltantes pueden ser **ignorables**. Pero la **tercera** requiere considerar **solo porciones de los datos** que no est칠n impactados o intentar **modelar los datos faltantes de alguna manera**.

Una forma de descubrir datos faltantes es usar la funci칩n `.info()` ya que indicar치 el **n칰mero de filas pero tambi칠n el n칰mero de valores por categor칤a**. Si alguna categor칤a tiene menos valores que el n칰mero de filas, entonces hay algunos datos faltantes:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Se recomienda generalmente que si una caracter칤stica **falta en m치s del 20%** del conjunto de datos, la **columna debe eliminarse:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Tenga en cuenta que **no todos los valores faltantes est치n ausentes en el conjunto de datos**. Es posible que a los valores faltantes se les haya dado el valor "Desconocido", "n/a", "", -1, 0... Necesita revisar el conjunto de datos (usando `dataset.column`_`name.value`_`counts(dropna=False)` para verificar los posibles valores).
{% endhint %}

Si falta informaci칩n en el conjunto de datos (y no es demasiado), necesita encontrar la **categor칤a de los datos faltantes**. Para eso b치sicamente necesita saber si los **datos faltantes son al azar o no**, y para eso necesita averiguar si los **datos faltantes estaban correlacionados con otros datos** del conjunto de datos.

Para averiguar si un valor faltante est치 correlacionado con otra columna, puede crear una nueva columna que ponga 1s y 0s si los datos faltan o no y luego calcular la correlaci칩n entre ellos:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Si decides ignorar los datos faltantes, a칰n necesitas decidir qu칠 hacer con ellos: Puedes **eliminar las filas** con datos faltantes (los datos de entrenamiento para el modelo ser치n menores), puedes **eliminar la caracter칤stica** por completo, o podr칤as **modelarla**.

Deber칤as **verificar la correlaci칩n entre la caracter칤stica faltante y la columna objetivo** para ver cu치n importante es esa caracter칤stica para el objetivo, si es realmente **peque침a** puedes **descartarla o rellenarla**.

Para rellenar datos **continuos** faltantes podr칤as usar: la **media**, la **mediana** o usar un algoritmo de **imputaci칩n**. El algoritmo de imputaci칩n puede intentar usar otras caracter칤sticas para encontrar un valor para la caracter칤stica faltante:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para completar datos categ칩ricos, primero debes pensar si hay alguna raz칩n por la que los valores est치n ausentes. Si es por **elecci칩n de los usuarios** (no quer칤an dar los datos), quiz치s puedas **crear una nueva categor칤a** que indique eso. Si es debido a un error humano, puedes **eliminar las filas** o la **caracter칤stica** (revisa los pasos mencionados antes) o **completarla con la moda, la categor칤a m치s utilizada** (no recomendado).

# Combinando Caracter칤sticas

Si encuentras **dos caracter칤sticas** que est치n **correlacionadas** entre s칤, generalmente deber칤as **eliminar** una de ellas (la que est치 menos correlacionada con el objetivo), pero tambi칠n podr칤as intentar **combinarlas y crear una nueva caracter칤stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>Aprende hacking en AWS de cero a h칠roe con</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>!</strong></summary>

Otras formas de apoyar a HackTricks:

* Si quieres ver a tu **empresa anunciada en HackTricks** o **descargar HackTricks en PDF** consulta los [**PLANES DE SUSCRIPCI칍N**](https://github.com/sponsors/carlospolop)!
* Consigue el [**merchandising oficial de PEASS & HackTricks**](https://peass.creator-spring.com)
* Descubre [**La Familia PEASS**](https://opensea.io/collection/the-peass-family), nuestra colecci칩n de [**NFTs**](https://opensea.io/collection/the-peass-family) exclusivos
* **칔nete al** 游눫 [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **sigue**me en **Twitter** 游냕 [**@carlospolopm**](https://twitter.com/carlospolopm)**.**
* **Comparte tus trucos de hacking enviando PRs a los repositorios de github de** [**HackTricks**](https://github.com/carlospolop/hacktricks) y [**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud).

</details>
