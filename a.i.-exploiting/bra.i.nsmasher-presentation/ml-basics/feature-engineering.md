<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>☁️ HackTricks Cloud ☁️</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>🐦 Twitter 🐦</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>🎙️ Twitch 🎙️</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>🎥 Youtube 🎥</strong></a></summary>

- **サイバーセキュリティ会社**で働いていますか？ **HackTricksで会社を宣伝**したいですか？または、**PEASSの最新バージョンにアクセスしたり、HackTricksをPDFでダウンロード**したいですか？[**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)をチェックしてください！

- [**The PEASS Family**](https://opensea.io/collection/the-peass-family)を見つけてください。独占的な[**NFT**](https://opensea.io/collection/the-peass-family)のコレクションです。

- [**公式のPEASS＆HackTricksのグッズ**](https://peass.creator-spring.com)を手に入れましょう。

- [**💬**](https://emojipedia.org/speech-balloon/) [**Discordグループ**](https://discord.gg/hRep4RUj7f)または[**telegramグループ**](https://t.me/peass)に**参加**するか、**Twitter**で**フォロー**してください[**🐦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**。**

- **ハッキングのトリックを共有するには、[hacktricksリポジトリ](https://github.com/carlospolop/hacktricks)と[hacktricks-cloudリポジトリ](https://github.com/carlospolop/hacktricks-cloud)にPRを提出してください**。

</details>


# 可能なデータの基本的なタイプ

データは**連続的**な（**無限**の値）または**カテゴリカル**（名義）であり、可能な値の数は**限られています**。

## カテゴリカルタイプ

### 2値

**2つの可能な値**のみ：1または0。データセットの値が文字列形式（例：「True」と「False」）の場合、これらの値に数値を割り当てます：
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **序数**

値は順序に従います。例えば、1位、2位...のようになります。もしカテゴリが文字列である場合（例: "初心者"、"アマチュア"、"プロフェッショナル"、"エキスパート"）、バイナリの場合と同様に数値にマッピングすることができます。
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* **アルファベットの列**の場合、それらをより簡単に並べ替えることができます：
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **循環的**

順序があるため、**序数値のように見えます**が、一方が他よりも大きいという意味ではありません。また、**それらの間の距離は、数える方向に依存します**。例：曜日、日曜日は月曜日よりも「大きい」わけではありません。

* 循環的な特徴をエンコードするための**さまざまな方法**がありますが、いくつかのアルゴリズムでは機能する場合もあります。**一般的には、ダミーエンコードが使用できます**。
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **日付**

日付は**連続的な**変数です。日付は**周期的**（繰り返されるため）または**順序的**変数として見ることができます（時間は前の時間よりも大きいため）。

* 通常、日付は**インデックス**として使用されます。
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### マルチカテゴリ/名義

**関連する順序のない**2つ以上のカテゴリ。各特徴のカテゴリに関する情報を取得するには、`dataset.describe(include='all')`を使用します。

* **参照文字列**は、**例を識別する列**です（例：人の名前）。これは重複する場合があります（2人が同じ名前を持つ場合があるため）、しかしほとんどはユニークです。このデータは**無用であり、削除する必要があります**。
* **キーカラム**は、テーブル間のデータを**リンクするために使用されます**。この場合、要素はユニークです。このデータは**無用であり、削除する必要があります**。

マルチカテゴリの列を数値に**エンコードするために**（MLアルゴリズムがそれらを理解するために）、**ダミーエンコーディングが使用されます**（そして**ワンホットエンコーディングではない**のは、**完全な多重共線性を回避しない**ためです）。

`pd.get_dummies(dataset.column1)`を使用して、**マルチカテゴリの列をワンホットエンコード**できます。これにより、すべてのクラスがバイナリ特徴に変換され、**可能なクラスごとに1つの新しい列**が作成され、1つの列には1つの**真の値が割り当てられ、他の列は偽になります**。

`pd.get_dummies(dataset.column1, drop_first=True)`を使用して、**マルチカテゴリの列をダミーエンコード**できます。これにより、すべてのクラスがバイナリ特徴に変換され、**可能なクラスごとに1つの新しい列マイナス1つ**が作成されます。最後の2つの列は、最後のバイナリ列で「1」または「0」として反映されます。これにより、完全な多重共線性が回避され、列間の関係が減少します。

# 共線性/多重共線性

共線性は、**2つの特徴が互いに関連している**場合に発生します。多重共線性は、それが2つ以上の場合に発生します。

MLでは、**特徴が可能な結果と関連していることを望みますが、それらが互いに関連していることは望みません**。そのため、**ダミーエンコーディングは最後の2つの列を混合**し、**ワンホットエンコーディングよりも優れています**。ワンホットエンコーディングは、マルチカテゴリの列から生成されたすべての新しい特徴間に明確な関係を作成しないためです。

VIFは**分散膨張係数**であり、**特徴の多重共線性を測定**します。値が**5を超える場合、2つ以上の共線性のある特徴のうち1つを削除する必要があります**。
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# カテゴリの不均衡

これは、トレーニングデータにおいて各カテゴリの数が**同じではない**場合に発生します。
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
不均衡なデータセットでは、常に**多数派クラスまたはクラス**と**少数派クラスまたはクラス**が存在します。

この問題を解決するためには、主に2つの方法があります：

* **アンダーサンプリング**：多数派クラスからランダムにデータを削除し、少数派クラスと同じサンプル数にします。
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **オーバーサンプリング**: マイノリティクラスのサンプル数が多数派クラスと同じ数になるまで、データを増やすこと。
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
引数**`sampling_strategy`**を使用して、**アンダーサンプリングまたはオーバーサンプリング**する**割合**を指定できます（**デフォルトでは1（100%）**で、少数クラスの数を多数クラスと同じにすることを意味します）。

{% hint style="info" %}
アンダーサンプリングまたはオーバーサンプリングは完璧ではありません。オーバー/アンダーサンプリングされたデータの統計（`.describe()`で取得）を元のデータと比較すると、**変化していることがわかります**。したがって、オーバーサンプリングとアンダーサンプリングはトレーニングデータを変更しています。
{% endhint %}

## SMOTEオーバーサンプリング

**SMOTE**は通常、データをオーバーサンプリングする**より信頼性の高い方法**です。
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# まれに発生するカテゴリ

ほとんどの場合、ターゲットクラスの1つが非常に少ない回数しか発生しないデータセットを想像してください。

これは、前のセクションのカテゴリの不均衡と似ていますが、まれに発生するカテゴリはその場合の「少数派クラス」よりもさらに少なく発生しています。ここでも、**生の** **オーバーサンプリング**と**アンダーサンプリング**の手法を使用することができますが、一般的にはこれらの手法は**本当に良い結果を得ることはありません**。

## 重み

一部のアルゴリズムでは、モデルを生成する際に、ターゲットデータの重みを変更することが可能です。これにより、いくつかのデータがデフォルトでより重要視されるようになります。
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
**重みをオーバーサンプリング/アンダーサンプリングのテクニックと組み合わせることで、結果を改善することができます。**

## PCA - 主成分分析

これはデータの次元を削減するのに役立つ手法です。異なる特徴を組み合わせて、より有用な特徴を生成し、データの量を減らします（計算量が少なくなります）。

生成された特徴は人間には理解できないため、データを匿名化する役割も果たします。

# 不整合なラベルカテゴリ

データには、変換が失敗した場合やデータの記述時の人為的なエラーにより、間違いが含まれる可能性があります。

そのため、同じラベルについて、スペルミスがあったり、大文字小文字が異なったり、略語が使われたりすることがあります。例えば、_BLUE, Blue, b, bule_のようなものです。モデルのトレーニングの前に、これらのラベルのエラーを修正する必要があります。

すべてのデータが正しくラベル付けされていることを確認することは非常に重要です。なぜなら、例えばデータのスペルミスがある場合、クラスをダミーエンコーディングすると、最終的な特徴に誤った結果をもたらす新しい列が生成されるからです。この例は、1つの列をワンホットエンコーディングし、作成された列の名前を確認することで非常に簡単に検出できます。

# 欠損データ

研究データの一部が欠損している場合があります。

完全にランダムなデータがエラーのために欠落している場合があります。これは、**完全にランダムに欠損している**（**MCAR**）と呼ばれるものです。

ランダムなデータが欠落しているが、特定の詳細が欠落する可能性が高くなる要因がある場合もあります。例えば、男性は年齢を教えることが多いが、女性は教えないことがよくあります。これは**ランダムに欠損している**（**MAR**）と呼ばれます。

最後に、データが**ランダムに欠損していない**（**MNAR**）場合があります。データの値は、データの存在確率と直接関連しています。例えば、何か恥ずかしいことを測定したい場合、その人が恥ずかしいと思うほど、それを共有する可能性は低くなります。

**最初の2つのカテゴリ**の欠損データは**無視できる**場合があります。しかし、**3番目のカテゴリ**では、影響を受けないデータのみを考慮するか、欠損データを何らかの方法でモデル化する必要があります。

欠損データについて調べる方法の1つは、`.info()`関数を使用することです。この関数は、行数だけでなく、カテゴリごとの値の数も示してくれます。もしカテゴリごとの値の数が行数よりも少ない場合、データが欠損していることがあります。
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
通常、データセットの20%以上で特徴が欠落している場合、その**列は削除することが推奨されます:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
データセットには、**すべての欠損値が欠損しているわけではありません**。欠損値には、"Unknown"、"n/a"、""、-1、0などの値が与えられている可能性があります。データセットを確認する必要があります（`dataset.column_name.value_counts(dropna=False)`を使用して可能な値を確認します）。
{% endhint %}

データセットにデータが欠損している場合（それほど多くない場合）、**欠損データのカテゴリ**を見つける必要があります。そのためには、基本的には**欠損データがランダムかどうかを知る必要があり**、そのためには、データセットの他のデータと**欠損データが相関しているかどうかを見つける必要があります**。

欠損値が他の列と相関しているかどうかを見つけるには、データが欠損しているかどうかに応じて1と0を設定する新しい列を作成し、それらの間の相関を計算することができます：
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
もし欠損データを無視することを決めた場合、それに対して何をする必要があります。欠損データを持つ行を**削除**することができます（モデルのトレーニングデータが小さくなります）、または**特徴量を完全に削除**するか、**モデル化**することもできます。

欠損している特徴量とターゲット列の**相関関係をチェック**して、その特徴量がターゲットにとってどれだけ重要かを確認する必要があります。もし相関が**非常に小さい**場合は、それを**削除するか埋める**ことができます。

欠損している**連続データ**を埋めるためには、**平均値**、**中央値**、または**補完アルゴリズム**を使用することができます。補完アルゴリズムは、他の特徴量を使用して欠損している特徴量の値を見つけることを試みることができます。
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
カテゴリデータを埋めるためには、まず欠損している理由があるか考える必要があります。もしユーザーの**選択**によるものであれば（データを提供したくなかった場合）、新しいカテゴリを作成してそれを示すことができます。もし人為的なエラーによるものであれば、行または特徴量を**削除**するか（前述の手順を確認してください）、もしくは最も使用されているカテゴリである**モードで埋める**ことができます（おすすめではありません）。

# 特徴量の結合

もし**2つの特徴量**が**相関している**場合、通常はそれらのうち相関がターゲットとより低い方を**削除**するべきですが、それらを結合して新しい特徴量を作成することも試すことができます。
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>☁️ HackTricks Cloud ☁️</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>🐦 Twitter 🐦</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>🎙️ Twitch 🎙️</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>🎥 Youtube 🎥</strong></a></summary>

- **サイバーセキュリティ会社**で働いていますか？ **HackTricksで会社を宣伝**したいですか？または、**PEASSの最新バージョンにアクセスしたり、HackTricksをPDFでダウンロード**したいですか？[**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)をチェックしてください！

- [**The PEASS Family**](https://opensea.io/collection/the-peass-family)を見つけてください。独占的な[**NFT**](https://opensea.io/collection/the-peass-family)のコレクションです。

- [**公式のPEASS＆HackTricksのグッズ**](https://peass.creator-spring.com)を手に入れましょう。

- [**💬**](https://emojipedia.org/speech-balloon/) [**Discordグループ**](https://discord.gg/hRep4RUj7f)または[**telegramグループ**](https://t.me/peass)に**参加**するか、**Twitter**で**フォロー**してください[**🐦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**.**

- **ハッキングのトリックを共有するには、[hacktricksリポジトリ](https://github.com/carlospolop/hacktricks)と[hacktricks-cloudリポジトリ](https://github.com/carlospolop/hacktricks-cloud)**にPRを提出してください。

</details>
