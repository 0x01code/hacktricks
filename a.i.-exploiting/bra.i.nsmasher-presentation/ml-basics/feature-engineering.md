<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>‚òÅÔ∏è HackTricks Cloud ‚òÅÔ∏è</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>üê¶ Twitter üê¶</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>üéôÔ∏è Twitch üéôÔ∏è</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>üé• Youtube üé•</strong></a></summary>

- ¬øTrabajas en una **empresa de ciberseguridad**? ¬øQuieres ver tu **empresa anunciada en HackTricks**? ¬øO quieres tener acceso a la **√∫ltima versi√≥n de PEASS o descargar HackTricks en PDF**? ¬°Mira los [**PLANES DE SUSCRIPCI√ìN**](https://github.com/sponsors/carlospolop)!

- Descubre [**The PEASS Family**](https://opensea.io/collection/the-peass-family), nuestra colecci√≥n exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)

- Consigue la [**oficial PEASS & HackTricks swag**](https://peass.creator-spring.com)

- **√önete al** [**üí¨**](https://emojipedia.org/speech-balloon/) [**grupo de Discord**](https://discord.gg/hRep4RUj7f) o al [**grupo de telegram**](https://t.me/peass) o **s√≠gueme** en **Twitter** [**üê¶**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**.**

- **Comparte tus trucos de hacking enviando PRs al [repositorio de hacktricks](https://github.com/carlospolop/hacktricks) y al [repositorio de hacktricks-cloud](https://github.com/carlospolop/hacktricks-cloud)**.

</details>


# Tipos b√°sicos de datos posibles

Los datos pueden ser **continuos** (con **infinitos** valores) o **categ√≥ricos** (nominales) donde la cantidad de valores posibles es **limitada**.

## Tipos categ√≥ricos

### Binario

Solo hay **2 valores posibles**: 1 o 0. En caso de que en un conjunto de datos los valores est√©n en formato de cadena (por ejemplo, "Verdadero" y "Falso"), se asignan n√∫meros a esos valores con:
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **Ordinal**

Los **valores siguen un orden**, como en: 1er lugar, 2do lugar... Si las categor√≠as son cadenas de texto (como: "principiante", "amateur", "profesional", "experto") se pueden asignar n√∫meros a cada una de ellas como vimos en el caso binario.
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* Para las columnas **alfab√©ticas** puedes ordenarlas m√°s f√°cilmente:
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **C√≠clico**

Se parece a un valor ordinal porque hay un orden, pero no significa que uno sea m√°s grande que el otro. Adem√°s, la distancia entre ellos depende de la direcci√≥n en la que se est√© contando. Ejemplo: los d√≠as de la semana, el domingo no es "m√°s grande" que el lunes.

* Hay diferentes formas de codificar caracter√≠sticas c√≠clicas, algunas pueden funcionar solo con algunos algoritmos. En general, se puede utilizar la codificaci√≥n de variables ficticias.
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **Fechas**

Las fechas son **variables continuas**. Pueden ser vistas como **c√≠clicas** (porque se repiten) o como variables **ordinales** (porque un tiempo es mayor que otro anterior).

* Usualmente las fechas son usadas como **√≠ndice**.
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### Multi-categor√≠a/nominal

**M√°s de 2 categor√≠as** sin un orden relacionado. Use `dataset.describe(include='all')` para obtener informaci√≥n sobre las categor√≠as de cada caracter√≠stica.

* Una **cadena de referencia** es una **columna que identifica un ejemplo** (como el nombre de una persona). Esto puede estar duplicado (porque 2 personas pueden tener el mismo nombre), pero la mayor√≠a ser√° √∫nico. Estos datos son **in√∫tiles y deben eliminarse**.
* Una **columna clave** se utiliza para **vincular datos entre tablas**. En este caso, los elementos son √∫nicos. Estos datos son **in√∫tiles y deben eliminarse**.

Para **codificar columnas de m√∫ltiples categor√≠as en n√∫meros** (para que el algoritmo de ML los entienda), se utiliza la **codificaci√≥n de dummies** (y **no la codificaci√≥n one-hot** porque **no evita la multicolinealidad perfecta**).

Puede obtener una **columna de m√∫ltiples categor√≠as codificada one-hot** con `pd.get_dummies(dataset.column1)`. Esto transformar√° todas las clases en caracter√≠sticas binarias, por lo que crear√° **una nueva columna por cada clase posible** y asignar√° 1 **valor verdadero a una columna**, y el resto ser√° falso.

Puede obtener una **columna de m√∫ltiples categor√≠as codificada en dummies** con `pd.get_dummies(dataset.column1, drop_first=True)`. Esto transformar√° todas las clases en caracter√≠sticas binarias, por lo que crear√° **una nueva columna por cada clase posible menos una** ya que **las √∫ltimas 2 columnas se reflejar√°n como "1" o "0" en la √∫ltima columna binaria creada**. Esto evitar√° la multicolinealidad perfecta, reduciendo las relaciones entre columnas.

# Colinealidad/Multicolinealidad

La colinealidad aparece cuando **2 caracter√≠sticas est√°n relacionadas entre s√≠**. La multicolinealidad aparece cuando hay m√°s de 2.

En ML **quieres que tus caracter√≠sticas est√©n relacionadas con los posibles resultados, pero no quieres que est√©n relacionadas entre s√≠**. Es por eso que la **codificaci√≥n de dummies mezcla las √∫ltimas dos columnas** de eso y **es mejor que la codificaci√≥n one-hot** que no lo hace, creando una clara relaci√≥n entre todas las nuevas caracter√≠sticas de la columna de m√∫ltiples categor√≠as.

VIF es el **Factor de Inflaci√≥n de la Varianza** que **mide la multicolinealidad de las caracter√≠sticas**. Un valor **superior a 5 significa que una de las dos o m√°s caracter√≠sticas colineales debe eliminarse**.
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# Desequilibrio Categ√≥rico

Esto ocurre cuando **no hay la misma cantidad de cada categor√≠a** en los datos de entrenamiento.
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
En un desequilibrio siempre hay una **clase o clases mayoritarias** y una **clase o clases minoritarias**.

Hay 2 formas principales de solucionar este problema:

* **Submuestreo**: Eliminar datos seleccionados al azar de la clase mayoritaria para que tenga el mismo n√∫mero de muestras que la clase minoritaria.
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **Sobremuestreo**: Generar m√°s datos para la clase minoritaria hasta que tenga tantas muestras como la clase mayoritaria.
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
Puedes usar el argumento **`sampling_strategy`** para indicar el **porcentaje** que deseas **submuestrear o sobremuestrear** (**por defecto es 1 (100%)** lo que significa igualar el n√∫mero de clases minoritarias con las clases mayoritarias).

{% hint style="info" %}
El submuestreo o sobremuestreo no son perfectos, si obtienes estad√≠sticas (con `.describe()`) de los datos sobre/submuestreados y los comparas con los originales, ver√°s **que han cambiado**. Por lo tanto, el sobremuestreo y el submuestreo modifican los datos de entrenamiento.
{% endhint %}

## Sobremuestreo SMOTE

**SMOTE** es generalmente una **forma m√°s confiable de sobremuestrear los datos**.
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# Categor√≠as de ocurrencia rara

Imagina un conjunto de datos donde una de las clases objetivo ocurre muy pocas veces.

Esto es similar al desequilibrio de categor√≠as de la secci√≥n anterior, pero la categor√≠a de ocurrencia rara ocurre incluso menos que la "clase minoritaria" en ese caso. Los m√©todos de **sobremuestreo** y **submuestreo** **brutos** tambi√©n podr√≠an usarse aqu√≠, pero generalmente esas t√©cnicas **no dar√°n resultados realmente buenos**.

## Pesos

En algunos algoritmos es posible **modificar los pesos de los datos objetivo** para que algunos de ellos tengan por defecto m√°s importancia al generar el modelo.
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
Puedes **mezclar los pesos con t√©cnicas de sobremuestreo/submuestreo** para intentar mejorar los resultados.

## PCA - An√°lisis de Componentes Principales

Es un m√©todo que ayuda a reducir la dimensionalidad de los datos. Va a **combinar diferentes caracter√≠sticas** para **reducir la cantidad** de ellas generando **caracter√≠sticas m√°s √∫tiles** (_se necesita menos c√≥mputo_).

Las caracter√≠sticas resultantes no son comprensibles por los humanos, por lo que tambi√©n **anonimiza los datos**.

# Categor√≠as de etiquetas incongruentes

Los datos pueden tener errores por transformaciones fallidas o simplemente por errores humanos al escribir los datos.

Por lo tanto, es posible encontrar la **misma etiqueta con errores ortogr√°ficos**, diferentes **may√∫sculas**, **abreviaturas** como: _BLUE, Blue, b, bule_. Necesitas corregir estos errores de etiqueta dentro de los datos antes de entrenar el modelo.

Puedes solucionar estos problemas convirtiendo todo en min√∫sculas y asignando etiquetas mal escritas a las correctas.

Es muy importante comprobar que **todos los datos que tienes est√°n etiquetados correctamente**, porque por ejemplo, un error de ortograf√≠a en los datos, al codificar las clases, generar√° una nueva columna en las caracter√≠sticas finales con **consecuencias negativas para el modelo final**. Este ejemplo se puede detectar muy f√°cilmente codificando en caliente una columna y comprobando los nombres de las columnas creadas.

# Datos faltantes

Puede faltar alg√∫n dato del estudio.

Puede suceder que falte alg√∫n dato completamente al azar por alg√∫n error. Este tipo de dato est√° **Completamente Faltante al Azar** (**MCAR**).

Podr√≠a ser que falte alg√∫n dato al azar, pero hay algo que hace que algunos detalles espec√≠ficos sean m√°s probables de faltar, por ejemplo, los hombres suelen decir su edad con m√°s frecuencia que las mujeres. Esto se llama **Faltante al Azar** (**MAR**).

Finalmente, podr√≠a haber datos **Faltantes No al Azar** (**MNAR**). El valor de los datos est√° directamente relacionado con la probabilidad de tener los datos. Por ejemplo, si quieres medir algo vergonzoso, cuanto m√°s vergonzoso sea alguien, menos probable es que lo comparta.

Las **dos primeras categor√≠as** de datos faltantes se pueden **ignorar**. Pero la **tercera** requiere considerar **s√≥lo porciones de los datos** que no est√©n afectadas o intentar **modelar los datos faltantes de alguna manera**.

Una forma de averiguar sobre los datos faltantes es usar la funci√≥n `.info()`, ya que indicar√° el **n√∫mero de filas pero tambi√©n el n√∫mero de valores por categor√≠a**. Si alguna categor√≠a tiene menos valores que el n√∫mero de filas, entonces faltan algunos datos:
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
Generalmente se recomienda que si una caracter√≠stica **falta en m√°s del 20%** del conjunto de datos, la **columna debe ser eliminada:**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
Ten en cuenta que **no todos los valores faltantes est√°n ausentes en el conjunto de datos**. Es posible que los valores faltantes hayan sido reemplazados por "Desconocido", "n/a", "", -1, 0... Debes verificar el conjunto de datos (usando `dataset.column`_`name.value`_`counts(dropna=False)` para verificar los posibles valores).
{% endhint %}

Si falta alg√∫n dato en el conjunto de datos (si no es demasiado), debes encontrar la **categor√≠a de los datos faltantes**. Para ello, b√°sicamente necesitas saber si los **datos faltantes est√°n al azar o no**, y para ello necesitas encontrar si los **datos faltantes estaban correlacionados con otros datos** del conjunto de datos.

Para encontrar si un valor faltante est√° correlacionado con otra columna, puedes crear una nueva columna que ponga 1 y 0 si los datos faltan o no, y luego calcular la correlaci√≥n entre ellos:
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
Si decides ignorar los datos faltantes, a√∫n necesitas decidir qu√© hacer con ellos: puedes **eliminar las filas** con datos faltantes (los datos de entrenamiento para el modelo ser√°n m√°s peque√±os), puedes **eliminar completamente la caracter√≠stica**, o puedes **modelarla**.

Debes **verificar la correlaci√≥n entre la caracter√≠stica faltante y la columna objetivo** para ver qu√© tan importante es esa caracter√≠stica para el objetivo, si es realmente **peque√±a**, puedes **eliminarla o llenarla**.

Para llenar datos faltantes **continuos**, puedes usar: la **media**, la **mediana** o usar un **algoritmo de imputaci√≥n**. El algoritmo de imputaci√≥n puede intentar usar otras caracter√≠sticas para encontrar un valor para la caracter√≠stica faltante:
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
Para rellenar datos categ√≥ricos, primero debes pensar si hay alguna raz√≥n por la cual los valores est√©n faltando. Si es por **elecci√≥n de los usuarios** (no quisieron proporcionar los datos), tal vez puedas **crear una nueva categor√≠a** que lo indique. Si es debido a un error humano, puedes **eliminar las filas** o la **caracter√≠stica** (verifica los pasos mencionados anteriormente) o **rellenarla con la moda, la categor√≠a m√°s utilizada** (no recomendado).

# Combinando Caracter√≠sticas

Si encuentras **dos caracter√≠sticas** que est√°n **correlacionadas** entre s√≠, generalmente deber√≠as **eliminar** una de ellas (la que est√° menos correlacionada con el objetivo), pero tambi√©n podr√≠as intentar **combinarlas y crear una nueva caracter√≠stica**.
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><a href="https://cloud.hacktricks.xyz/pentesting-cloud/pentesting-cloud-methodology"><strong>‚òÅÔ∏è HackTricks Cloud ‚òÅÔ∏è</strong></a> -<a href="https://twitter.com/hacktricks_live"><strong>üê¶ Twitter üê¶</strong></a> - <a href="https://www.twitch.tv/hacktricks_live/schedule"><strong>üéôÔ∏è Twitch üéôÔ∏è</strong></a> - <a href="https://www.youtube.com/@hacktricks_LIVE"><strong>üé• Youtube üé•</strong></a></summary>

- ¬øTrabajas en una **empresa de ciberseguridad**? ¬øQuieres ver tu **empresa anunciada en HackTricks**? ¬øO quieres tener acceso a la **√∫ltima versi√≥n de PEASS o descargar HackTricks en PDF**? ¬°Revisa los [**PLANES DE SUSCRIPCI√ìN**](https://github.com/sponsors/carlospolop)!

- Descubre [**The PEASS Family**](https://opensea.io/collection/the-peass-family), nuestra colecci√≥n exclusiva de [**NFTs**](https://opensea.io/collection/the-peass-family)

- Obt√©n el [**swag oficial de PEASS y HackTricks**](https://peass.creator-spring.com)

- **√önete al** [**üí¨**](https://emojipedia.org/speech-balloon/) **grupo de Discord** o al [**grupo de telegram**](https://t.me/peass) o **s√≠gueme en** **Twitter** [**üê¶**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/hacktricks_live)**.**

- **Comparte tus trucos de hacking enviando PRs al [repositorio de hacktricks](https://github.com/carlospolop/hacktricks) y al [repositorio de hacktricks-cloud](https://github.com/carlospolop/hacktricks-cloud)**.

</details>
