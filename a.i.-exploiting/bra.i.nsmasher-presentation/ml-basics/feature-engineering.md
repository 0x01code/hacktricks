<details>

<summary><strong>AWSハッキングをゼロからヒーローまで学ぶ</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>！</strong></summary>

HackTricksをサポートする他の方法:

* **HackTricksにあなたの会社を広告したい場合**、または**HackTricksをPDFでダウンロードしたい場合**は、[**サブスクリプションプラン**](https://github.com/sponsors/carlospolop)をチェックしてください！
* [**公式PEASS & HackTricksグッズ**](https://peass.creator-spring.com)を入手する
* [**The PEASS Family**](https://opensea.io/collection/the-peass-family)を発見し、独占的な[**NFTs**](https://opensea.io/collection/the-peass-family)のコレクションをチェックする
* 💬 [**Discordグループ**](https://discord.gg/hRep4RUj7f)に**参加する**か、[**テレグラムグループ**](https://t.me/peass)に参加するか、**Twitter** 🐦 [**@carlospolopm**](https://twitter.com/carlospolopm)を**フォローする**。
* **HackTricks**の[**GitHubリポジトリ**](https://github.com/carlospolop/hacktricks)と[**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud)にPRを提出して、あなたのハッキングのコツを共有する。

</details>


# 可能なデータの基本的なタイプ

データは**連続的**（**無限**の値）または**カテゴリカル**（名義）で、可能な値の量が**限定**されています。

## カテゴリカルなタイプ

### バイナリ

ただ**2つの可能な値**：1または0。データセットで値が文字列形式（例："True"と"False"）の場合、次のように数値を割り当てます：
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **序数**

**値には順序があります**。例えば、1位、2位...のように。カテゴリが文字列の場合（例："starter", "amateur", "professional", "expert"）、バイナリケースで見たように、それらを数字にマッピングすることができます。
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* **アルファベット列**では、より簡単に並べ替えることができます：
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **周期的**

順序があるように**見える序数値**ですが、一方が他方より大きいという意味ではありません。また、数える**方向によって距離が異なります**。例：週の日々、日曜日は月曜日より「大きい」わけではありません。

* 周期的特徴をエンコードする**異なる方法**があり、一部のアルゴリズムでのみ機能する場合があります。**一般的に、ダミーエンコードが使用できます**
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **日付**

日付は**連続する**変数です。繰り返されるため**周期的**と見なすことができます**または**時間が前のものより大きいため、**順序**変数と見なすこともできます。

* 通常、日付は**インデックス**として使用されます
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### 多カテゴリー/名義

**2つ以上のカテゴリー**で、関連する順序がない。`dataset.describe(include='all')`を使用して、各特徴のカテゴリーについての情報を取得します。

* **参照文字列**は**例を識別する列**です（人の名前のような）。これは重複することがあります（2人が同じ名前を持つ可能性があるため）が、ほとんどはユニークです。このデータは**無用であり、削除されるべきです**。
* **キーカラム**は、**テーブル間のデータをリンクするために使用されます**。この場合、要素はユニークです。このデータは**無用であり、削除されるべきです**。

**多カテゴリー列を数値にエンコードする**ために（MLアルゴリズムが理解できるように）、**ダミーエンコーディングが使用されます**（**ワンホットエンコーディングは使用されません**。なぜなら、それは**完全多重共線性を避けることができない**からです）。

`pd.get_dummies(dataset.column1)`を使用して、**多カテゴリー列をワンホットエンコード**することができます。これにより、すべてのクラスがバイナリ特徴に変換され、**可能なクラスごとに新しい列が1つ作成されます**。そして、1つの列に**True値が割り当てられ**、残りはfalseになります。

`pd.get_dummies(dataset.column1, drop_first=True)`を使用して、**多カテゴリー列をダミーエンコード**することができます。これにより、すべてのクラスがバイナリ特徴に変換され、**可能なクラスごとに新しい列が1つ作成されますが、最後の1つを除きます**。なぜなら、**最後の2列は最後に作成されたバイナリ列で「1」または「0」として反映される**からです。これにより、完全多重共線性を避け、列間の関係を減らします。

# 共線性/多重共線性

共線性は、**2つの特徴が互いに関連している**場合に現れます。多重共線性は、それらが2つ以上の場合に現れます。

MLでは、**特徴が可能な結果と関連していることを望みますが、それらが互いに関連していることは望みません**。そのため、**ダミーエンコーディングは最後の2列を混合し**、**ワンホットエンコーディングよりも優れています**。ワンホットエンコーディングはそれを行わず、多カテゴリー列のすべての新しい特徴間に明確な関係を作り出します。

VIFは**分散膨張因子**であり、**特徴の多重共線性を測定します**。値が**5以上である場合、2つ以上の共線性のある特徴のうちの1つを削除する必要があります**。
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# カテゴリの不均衡

これは、トレーニングデータにおいて**各カテゴリの量が同じでない**場合に発生します。
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
不均衡には常に**多数クラス**と**少数クラス**が存在します。

この問題を解決する主な方法は2つあります：

* **アンダーサンプリング**：多数クラスからランダムに選ばれたデータを削除し、少数クラスと同じサンプル数にします。
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **オーバーサンプリング**: 少数クラスのデータを増やし、多数クラスと同じ数のサンプルを持つまで生成します。
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
**`sampling_strategy`** 引数を使用して、**アンダーサンプルまたはオーバーサンプル**したい**割合**を指定できます（**デフォルトは1（100%）**で、少数クラスの数を多数クラスの数に等しくすることを意味します）

{% hint style="info" %}
アンダーサンプリングまたはオーバーサンプリングは完璧ではありません。オーバー/アンダーサンプリングされたデータの統計（`.describe()`で取得）を元のデータと比較すると、**変更されていることがわかります。** したがって、オーバーサンプリングとアンダーサンプリングはトレーニングデータを変更しています。
{% endhint %}

## SMOTE オーバーサンプリング

**SMOTE** は通常、データをオーバーサンプリングする**より信頼性の高い方法**です。
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# 稀に発生するカテゴリ

想像してください、あるデータセットで目標クラスが**非常に少ない回数**で発生する場合を。

これは前のセクションのカテゴリの不均衡に似ていますが、稀に発生するカテゴリはその場合の「少数クラス」よりもさらに少なく発生します。**生の** **オーバーサンプリング**と**アンダーサンプリング**の方法もここで使用できますが、一般的にこれらの技術は**本当に良い結果をもたらすことはありません**。

## 重み

いくつかのアルゴリズムでは、目標とするデータの重みを**変更することが可能**です。そのため、デフォルトでいくつかのデータがモデル生成時により重要となります。
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
以下は、データの特徴量エンジニアリングに関するハッキング技術の本からの内容です。関連する英語テキストを日本語に翻訳し、同じマークダウンおよびHTML構文を保ちます。コード、ハッキング技術名、ハッキング用語、クラウド/SaaSプラットフォーム名（Workspace、aws、gcpなど）、'leak'という単語、ペネトレーションテスト、およびマークダウンタグなどの翻訳は行いません。また、翻訳とマークダウン構文以外の余分なものは追加しないでください。

---

**重みをオーバーサンプリング/アンダーサンプリング技術と組み合わせる**ことで、結果を改善しようと試みることができます。

## PCA - 主成分分析

データの次元を削減するのに役立つ方法です。**異なる特徴を組み合わせて**、それらの量を減らし、**より有用な特徴を生成します**（_計算が少なくて済む_）。

結果として得られる特徴は人間には理解できないため、データの**匿名化**も行います。

# 不整合なラベルカテゴリ

データには、変換が失敗したり、データ入力時の人為的なエラーなどにより、誤りが含まれていることがあります。

そのため、**同じラベルがスペルミス**であったり、異なる**大文字小文字**であったり、**略語**であったりすることがあります。例：_BLUE, Blue, b, bule_。モデルをトレーニングする前に、これらのラベルエラーをデータ内で修正する必要があります。

これらの問題をクリーンにする方法として、すべてを小文字にして、スペルミスのラベルを正しいものにマッピングすることができます。

**すべてのデータが正しくラベル付けされていることを確認することが非常に重要です**。なぜなら、たとえばデータのスペルミスが1つあると、クラスをダミーエンコーディングする際に、最終的な特徴に新しい列が生成され、**最終モデルに悪影響を及ぼす**からです。この例は、列をワンホットエンコーディングして作成された列の名前をチェックすることで非常に簡単に検出できます。

# データの欠落

研究の一部のデータが欠落している可能性があります。

何らかのエラーで完全にランダムなデータが欠落していることがあります。この種のデータは**完全にランダムに欠落**（**MCAR**）です。

ランダムなデータが欠落しているが、特定の詳細が欠落しやすい何かがある場合もあります。例えば、男性は年齢を教えることが多いが、女性はそうではない場合です。これは**ランダムに欠落**（**MAR**）と呼ばれます。

最後に、**ランダムではなく欠落**（**MNAR**）しているデータがあります。データの値は、データを持っている確率と直接関連しています。例えば、恥ずかしいことを測定したい場合、人が恥ずかしさを感じるほど、それを共有する可能性は低くなります。

欠落データの**最初の2つのカテゴリ**は**無視可能**です。しかし、**3つ目のカテゴリ**は、影響を受けていないデータの**一部分のみを考慮する**か、何らかの方法で**欠落データをモデル化しようとする**必要があります。

欠落データについて知る一つの方法は、`.info()`関数を使用することです。これは**行数だけでなく、カテゴリごとの値の数も示します**。あるカテゴリの値が行数より少ない場合、データが欠落していることを意味します：
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
データセットの**20%以上で特徴が**欠落している場合は、**列を削除することが推奨されます：**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
データセットにすべての欠損値が含まれているわけではないことに注意してください。欠損値が「Unknown」、「n/a」、「」、-1、0...などの値で与えられている可能性があります。データセットをチェックし（`dataset.column`_`name.value`_`counts(dropna=False)`を使用して可能な値を確認する必要があります）。
{% endhint %}

データセットにデータが欠けている場合（それが多すぎない場合）、欠けているデータの**カテゴリーを見つける**必要があります。そのためには、欠けているデータが**ランダムかどうか**、そしてそれを知るためには、欠けているデータがデータセットの他のデータと**相関していたかどうか**を見つける必要があります。

欠損値が別の列と相関しているかどうかを見つけるために、データが欠けているかどうかに応じて1と0を置く新しい列を作成し、それらの間の相関を計算できます：
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
欠損データを無視することにした場合でも、それに対して何をする必要があります：欠損データがある**行を削除する**ことができます（モデルのトレーニングデータが小さくなります）、**特徴を完全に削除する**ことができます、または**モデル化する**ことができます。

欠損している特徴とターゲット列との**相関を確認する**べきです。その特徴がターゲットにとってどれほど重要かを見るために、もし本当に**小さい**場合は、それを**削除するか、または埋める**ことができます。

欠損している**連続データ**を埋めるためには、**平均**、**中央値**を使用するか、**代入**アルゴリズムを使用することができます。代入アルゴリズムは、他の特徴を使用して欠損している特徴の値を見つけることができます：
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
カテゴリカルデータを埋めるには、まず値がなぜ欠けているのかを考える必要があります。もし**ユーザーの選択**（データを提供したくなかった）によるものであれば、それを示す**新しいカテゴリを作成**することができます。もし人為的なエラーによるものであれば、**行を削除**するか、**特徴を削除**する（前に述べた手順を確認してください）、または**最も多く使用されているカテゴリであるモードで埋める**ことができます（推奨されません）。

# 特徴の組み合わせ

もし**2つの特徴**が相互に**相関している**ことがわかった場合、通常はそのうちの1つ（ターゲットとの相関が低い方）を**削除**すべきですが、**組み合わせて新しい特徴を作成**することも試みることができます。
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>AWSハッキングをゼロからヒーローまで学ぶ</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE (HackTricks AWS Red Team Expert)</strong></a><strong>！</strong></summary>

HackTricksをサポートする他の方法:

* **HackTricksにあなたの会社を広告したい場合**、または**HackTricksをPDFでダウンロードしたい場合**は、[**サブスクリプションプラン**](https://github.com/sponsors/carlospolop)をチェックしてください！
* [**公式PEASS & HackTricksグッズ**](https://peass.creator-spring.com)を入手する
* [**The PEASS Family**](https://opensea.io/collection/the-peass-family)を発見し、独占的な[**NFTs**](https://opensea.io/collection/the-peass-family)のコレクションをチェックする
* 💬 [**Discordグループ**](https://discord.gg/hRep4RUj7f)に**参加する**か、[**テレグラムグループ**](https://t.me/peass)に参加するか、**Twitter** 🐦 [**@carlospolopm**](https://twitter.com/carlospolopm)を**フォローする**。
* **HackTricks**の[**GitHubリポジトリ**](https://github.com/carlospolop/hacktricks)と[**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud)にPRを提出して、あなたのハッキングのコツを共有する。

</details>
