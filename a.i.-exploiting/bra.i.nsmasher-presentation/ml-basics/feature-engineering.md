<details>

<summary><strong>从零开始学习AWS黑客技术，成为专家</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE（HackTricks AWS红队专家）</strong></a><strong>！</strong></summary>

其他支持HackTricks的方式：

* 如果您想看到您的**公司在HackTricks中做广告**或**下载PDF格式的HackTricks**，请查看[**订阅计划**](https://github.com/sponsors/carlospolop)!
* 获取[**官方PEASS & HackTricks周边产品**](https://peass.creator-spring.com)
* 探索[**PEASS家族**](https://opensea.io/collection/the-peass-family)，我们的独家[**NFTs**](https://opensea.io/collection/the-peass-family)
* **加入** 💬 [**Discord群组**](https://discord.gg/hRep4RUj7f) 或 [**电报群组**](https://t.me/peass) 或 **关注**我们的**Twitter** 🐦 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**。**
* 通过向[**HackTricks**](https://github.com/carlospolop/hacktricks)和[**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github仓库提交PR来分享您的黑客技巧。

</details>


# 可能数据的基本类型

数据可以是**连续的**（**无限**个值）或**分类的**（名义），其中可能的值的数量是**有限**的。

## 分类类型

### 二元

只有**2个可能的值**：1或0。如果数据集中的值是字符串格式（例如"True"和"False"），您可以为这些值分配数字：
```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```
### **序数**

**数值遵循一定顺序**，比如：第1名，第2名... 如果类别是字符串（比如："初学者"，"业余爱好者"，"专业人士"，"专家"），你可以将它们映射为数字，就像我们在二进制情况下看到的那样。
```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```
* 对于**字母列**，您可以更轻松地对其进行排序：
```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```
### **循环的**

看起来像序数值因为有一个顺序，但这并不意味着一个比另一个大。另外，它们之间的距离取决于你计数的方向。例如：一周中的每一天，星期日并不比星期一“大”。

- 有不同的方法来编码循环特征，有些可能只适用于某些算法。**通常情况下，可以使用虚拟编码**。
```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```
### **日期**

日期是**连续**的**变量**。可以被视为**循环**的（因为它们重复）**或**作为**序数**变量（因为一个时间比前一个时间大）。

* 通常日期被用作**索引**
```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```
### 多类别/名义

**超过2个类别**，没有相关顺序。使用 `dataset.describe(include='all')` 获取每个特征的类别信息。

* **引用字符串**是**标识示例的列**（如人名）。这可能是重复的（因为两个人可能有相同的名字），但大多数是唯一的。这些数据是**无用的，应该被移除**。
* **关键列**用于**链接表格之间的数据**。在这种情况下，元素是唯一的。这些数据是**无用的，应该被移除**。

为了**将多类别列编码为数字**（以便机器学习算法理解它们），使用**虚拟编码**（而不是独热编码，因为它**不能避免完美的多重共线性**）。

您可以使用 `pd.get_dummies(dataset.column1)` **对多类别列进行独热编码**。这将把所有类别转换为二进制特征，因此将为每个可能的类别创建**一个新列**，并将一个**True值分配给一个列**，其余将为false。

您可以使用 `pd.get_dummies(dataset.column1, drop_first=True)` **对多类别列进行虚拟编码**。这将把所有类别转换为二进制特征，因此将为每个可能的类别创建**一个新列减去一个**，因为**最后两列将在最后创建的二进制列中反映为"1"或"0"**。这将避免完美的多重共线性，减少列之间的关系。

# 共线性/多重共线性

当**2个特征彼此相关**时出现共线性。当这些特征超过2个时出现多重共线性。

在机器学习中，**您希望您的特征与可能的结果相关，但不希望它们之间相关**。这就是为什么**虚拟编码混合最后两列**，比独热编码更好，后者不会创建清晰的关系，而是创建了来自多类别列的所有新特征之间的明显关系。

VIF是**方差膨胀因子**，用于**衡量特征的多重共线性**。值**大于5意味着应该移除两个或多个共线特征中的一个**。
```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```
# 类别不平衡

这种情况发生在训练数据中**每个类别的数量不相同**时。
```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```
在不平衡的情况下，总会存在**多数类别**和**少数类别**。

解决这个问题有两种主要方法：

- **欠采样**：从多数类别中随机删除数据，使其样本数量与少数类别相同。
```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```
* **过采样**：为少数类生成更多数据，直到其样本数量与多数类相同。
```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```
您可以使用参数**`sampling_strategy`**来指示您想要**欠采样或过采样的百分比**（**默认为1（100%）**，这意味着将少数类的数量与多数类相等）

{% hint style="info" %}
如果您获取过/欠采样数据的统计信息（使用`.describe()`），并将其与原始数据进行比较，您会发现**它们已经改变**。因此，过采样和欠采样会修改训练数据。
{% endhint %}

## SMOTE 过采样

**SMOTE**通常是一种**更可靠的过采样数据的方法**。
```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```
# 很少出现的类别

想象一下一个数据集，其中一个目标类别**出现的次数非常少**。

这就像前一节中的类别不平衡问题，但是很少出现的类别甚至比那种情况下的“少数类”出现的次数还要少。**原始**的**过采样**和**欠采样**方法也可以在这里使用，但通常这些技术**不会给出真正好的结果**。

## 权重

在一些算法中，可以**修改目标数据的权重**，这样在生成模型时，一些数据会默认获得更多的重要性。
```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```
## 主成分分析（PCA）

是一种有助于减少数据维度的方法。它将**结合不同特征**以**减少**它们的数量，生成**更有用的特征**（_需要更少的计算_）。

生成的特征对人类来说是不可理解的，因此它还**对数据进行匿名化**。

# 不一致的标签类别

数据可能存在错误，可能是由于转换失败，也可能是由于人为错误。

因此，您可能会发现**相同标签存在拼写错误**、**不同大小写**、**缩写**，例如：_BLUE，Blue，b，bule。在训练模型之前，您需要修复数据中的这些标签错误。

您可以通过将所有内容转换为小写并将拼写错误的标签映射到正确的标签来清理这些问题。

非常重要的是要检查**您拥有的所有数据是否被正确标记**，因为例如，数据中的一个拼写错误，在对类别进行虚拟编码时，将在最终特征中生成一个新列，对最终模型会产生**不良后果**。通过对一个列进行独热编码并检查所创建列的名称，可以非常容易地检测到这种情况。

# 缺失数据

研究中可能会缺少一些数据。

可能会发生一些完全随机的数据丢失，这是一种**完全随机缺失**（**MCAR**）的情况。

也可能是一些随机数据丢失，但有一些因素使得某些特定细节更有可能丢失，例如更频繁地男性会告诉他们的年龄而女性不会。这被称为**随机缺失**（**MAR**）。

最后，可能存在**非随机缺失**（**MNAR**）的数据。数据的值与拥有数据的概率直接相关。例如，如果您想测量某些令人尴尬的事情，某人越尴尬，他分享的可能性就越小。

**前两类**缺失数据可以被**忽略**。但**第三类**需要考虑**只有部分未受影响的数据**或尝试**以某种方式对缺失数据进行建模**。

发现缺失数据的一种方法是使用`.info()`函数，因为它将指示**每个类别的值的数量**。如果某个类别的值少于行数，则存在一些数据缺失：
```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```
通常建议，如果数据集中**超过20%的数据缺失**，则应**删除该列：**
```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```
{% hint style="info" %}
请注意，**数据集中并非所有缺失值都是缺失的**。可能缺失值已被赋予值为"Unknown"、"n/a"、""、-1、0等。您需要检查数据集（使用`dataset.column_name.value_counts(dropna=False)`来检查可能的值）。
{% endhint %}

如果数据集中有一些数据缺失（而不是太多），您需要找到**缺失数据的类别**。基本上，您需要知道**缺失数据是否是随机的**，为此，您需要找出**缺失数据是否与数据集的其他数据相关**。

要找出缺失值是否与另一列相关联，您可以创建一个新列，如果数据缺失或不缺失，则将1和0放入该列，然后计算它们之间的相关性：
```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```
如果您决定忽略缺失数据，仍然需要处理它：您可以**删除**具有缺失数据的行（模型的训练数据将更少），您可以完全**删除该特征**，或者可以**对其进行建模**。

您应该**检查缺失特征与目标列之间的相关性**，以查看该特征对目标的重要性如何，如果确实**很小**，您可以**删除它或填充它**。

要填补缺失的**连续数据**，您可以使用：**均值**、**中位数**或使用**填充**算法。填充算法可以尝试使用其他特征来找到缺失特征的值：
```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```
为了填补分类数据，首先需要考虑数值缺失的原因。如果是**用户选择**（他们不想提供数据），也许可以**创建一个新类别**来指示这一点。如果是因为人为错误，可以**删除行**或**特征**（在之前提到的步骤中检查），或者用**众数填充，即最常用的类别**（不建议）。

# 合并特征

如果发现**两个特征**之间**相关**，通常应该**删除**其中一个（与目标相关性较低的那个），但也可以尝试**将它们合并并创建一个新特征**。
```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```
<details>

<summary><strong>从零开始学习AWS黑客技术，成为专家</strong> <a href="https://training.hacktricks.xyz/courses/arte"><strong>htARTE（HackTricks AWS红队专家）</strong></a><strong>！</strong></summary>

其他支持HackTricks的方式：

* 如果您想看到您的**公司在HackTricks中做广告**或**下载PDF格式的HackTricks**，请查看[**订阅计划**](https://github.com/sponsors/carlospolop)!
* 获取[**官方PEASS & HackTricks周边产品**](https://peass.creator-spring.com)
* 探索[**PEASS家族**](https://opensea.io/collection/the-peass-family)，我们的独家[**NFTs**](https://opensea.io/collection/the-peass-family)
* **加入** 💬 [**Discord群**](https://discord.gg/hRep4RUj7f) 或 [**电报群**](https://t.me/peass) 或 **关注**我们的**Twitter** 🐦 [**@hacktricks_live**](https://twitter.com/hacktricks_live)**。**
* 通过向[**HackTricks**](https://github.com/carlospolop/hacktricks)和[**HackTricks Cloud**](https://github.com/carlospolop/hacktricks-cloud) github仓库提交PR来分享您的黑客技巧。

</details>
